##this file is used to configure generator propertyes

hadoop.home=/opt/modules/hadoop-2.7.2-modified/hadoop-2.7.2
spark.home=/opt/modules/spark-1.6.1-bin-hadoop-2.6.0
hibench.home=/opt/modules/YarnBench/HiBench




jobs=hibench

user=admin


##run time(s)
runtime=300



hadoop.url = http://192.168.2.20:8088
##Hadoop Mapreduce job configure, Hadoop generator will randomly pick up one job to run
hadoop.jobs                     =wordcount,sort
hadoop.jobs.ratios              =1,2
# this means 1 ratio for wordcount, 2 ratio for sort. if no conf, just equal share
## we better do not put comment on the same line as conf info

## the configuration of wordcount benchmark
hadoop.jobs.wordcount.jars      = ./a.jar
hadoop.jobs.wordcount.inputs    = /input1,/input2
hadoop.jobs.wordcount.output    = /output
hadoop.jobs.wordcount.parameter = p1,p2,p3
hadoop.jobs.wordcount.keyvalues = k1:v1,k2:v2,k3:v3




##spark sql job configure
##  confuigure the possible path to store query
sparksql.jobs.path      = /opt/modules/apache-hive-2.0.0-bin/hive-testbench/sample-queries-tpch/

##  the final path to run query will be ./hive-test/query1
#sparksql.jobs          = tpch_query1.sql,tpch_query3.sql,tpch_query4.sql,tpch_query5.sql,tpch_query6.sql,tpch_query7.sql,tpch_query8.sql,tpch_query9.sql,tpch_query10.sql,tpch_query11.sql,tpch_query12.sql,tpch_query13.sql,tpch_query14.sql,tpch_query15.sql,tpch_query16.sql,tpch_query17.sql,tpch_query18.sql,tpch_query19.sql,

sparksql.jobs           = tpch_query1.sql,tpch_query3.sql
#sparksql.jobs.ratios   = 1
sparksql.jobs.keyvalues = --master:yarn, --jars:/opt/modules/spark-1.6.1-bin-hadoop-2.6.0/lib/mysql-connector-5.1.39-bin.jar, --database:tpch_flat_orc_10





##for hibench job, the only property needed is jobs to list all wanted application
##We assume the input data has been generated by user 

hibench.jobs           = wordcount,sort
hibench.jobs.types     = mapreduce,mapreduce
hibench.jobs.ratios    = 2,1








##configure for multiple generators
generators = OrderGenerator1, OrderGenerator2, PoissonGenerator1, PoissonGenerator2, CapacityGenerator

generator.OrderGenerator1.jobs       =hadoop
generator.OrderGenerator1.order      =true
generator.OrderGenerator1.round      =3
generator.OrderGenerator1.range      =18
generator.OrderGenerator1.jobs.ratio =1
generator.OrderGenerator1.queue      =default
generator.OrderGenerator1.parameters.interval = 20
#

generator.OrderGenerator2.jobs       =hadoop
generator.OrderGenerator2.order      =true
generator.OrderGenerator2.round      =3
generator.OrderGenerator2.range      =18
generator.OrderGenerator2.jobs.ratio =1
generator.OrderGenerator2.queue      =default
generator.OrderGenerator2.parameters.interval = 20


generator.PoissonGenerator1.jobs         =sparksql
generator.PoissonGenerator1.jobs.ratios  =1
generator.PoissonGenerator1.queue        =default
generator.PoissonGenerator1.parameters.interval = 1
generator.PoissonGenerator1.parameters.mean     = 1
# the above interval means how long we need to check it we need to submit a job
# the above mean means for poission distribution

generator.PoissonGenerator2.jobs         =sparksql
generator.PoissonGenerator2.jobs.ratios  =1
generator.PoissonGenerator2.queue        =default
generator.PoissonGenerator2.parameters.interval = 1
generator.PoissonGenerator2.parameters.mean     = 1


generator.CapacityGenerator.jobs        =hibench
generator.CapacityGenerator.jobs.ratio  =1
generator.CapacityGenerator.queue       =default
generator.CapacityGenerator.parameters.usedCapacity       = 50
generator.CapcaityGenerator.parameters.usedCapacity.slice = 0.3:70,0.4:80
## the above value means at the 30 percent of execution time, usedCapacity value is changed from 50 to 70
## we wait 10s to make our new submission effectively occupy the cluster resource
